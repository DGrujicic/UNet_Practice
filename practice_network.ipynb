{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "# from collections import OrderedDict\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvBlock(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, groups=1):    \n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias, groups=groups)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, pooling=True):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.conv1 = ConvBlock(self.in_channels, self.out_channels)\n",
    "        self.conv2 = ConvBlock(self.out_channels, self.out_channels)\n",
    "\n",
    "        if self.pooling:\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        if self.pooling:\n",
    "            x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, in_channels=3, depth=5, num_fc = 2, \n",
    "                 out_channels_first_block=64, img_size = 256):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels_first_block = out_channels_first_block\n",
    "        self.depth = depth\n",
    "        self.num_fc = num_fc\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Setting Up Blocks of Two Convolutions and One Max Pooling\n",
    "        self.blocks = []\n",
    "        self.fully_connecteds = []\n",
    "        \n",
    "        for i in range(depth):\n",
    "            in_channels_block = self.in_channels if i == 0 else out_channels_block\n",
    "            out_channels_block = self.out_channels_first_block*(2**i)\n",
    "            # pooling = True if i < depth-1 else False\n",
    "            # I pool on the last one too, just to reduce the dimensions a bit more, but normally I wouldn't do that\n",
    "            block = Block(in_channels_block, out_channels_block, pooling=True)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "        self.blocks = nn.ModuleList(self.blocks)\n",
    "        \n",
    "        # Setting Up the Fully Connected Layers\n",
    "        out_channels =self.blocks[self.depth-1].conv2.out_channels\n",
    "        out_img_size = self.img_size/(2**(self.depth)) # Would normally be self.depth-1 but poolings done after the last conv too\n",
    "        self.in_num_first_fc = int(out_img_size**2 * out_channels)\n",
    "        \n",
    "        for i in range(num_fc):\n",
    "            in_num_fc = self.in_num_first_fc if i == 0 else out_num_fc\n",
    "            out_num_fc = self.num_classes if i == num_fc-1 else int(in_num_fc/4)\n",
    "            fully_connected=nn.Linear(in_num_fc, out_num_fc)\n",
    "        # init.normal(fully_connected1.weight)\n",
    "        # init.normal(fully_connected2.weight)\n",
    "        # self.add_module('fully_connected', fully_connected)\n",
    "            self.fully_connecteds.append(fully_connected)\n",
    "        \n",
    "        self.fully_connecteds = nn.ModuleList(self.fully_connecteds)\n",
    "        self.initialize()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(module):\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            init.xavier_normal(module.weight)\n",
    "            init.constant(module.bias, 0)\n",
    "        if isinstance(module, nn.Linear):\n",
    "            init.normal(module.weight)\n",
    "            init.constant(module.bias, 0)\n",
    "\n",
    "    def initialize(self):\n",
    "        for i, module in enumerate(self.modules()):\n",
    "            self.weight_init(module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass Through Blocks\n",
    "        for i, module in enumerate(self.blocks):\n",
    "            x = module(x)  \n",
    "        # Pass Through The Fully Connected Layer\n",
    "        x = x.view(-1, self.in_num_first_fc)\n",
    "        # print(\"x {}\".format(x))\n",
    "        for i, module in enumerate(self.fully_connecteds):\n",
    "            x = module(x)\n",
    "            x = F.relu(x) if i<self.num_fc-1 else x\n",
    "            # print(\"x{} {}\".format(i,x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bisect import bisect_right\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class _LRScheduler(object):\n",
    "    def __init__(self, optimizer, last_epoch=-1):\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "        if last_epoch == -1:\n",
    "            for group in optimizer.param_groups:\n",
    "                group.setdefault('initial_lr', group['lr'])\n",
    "        else:\n",
    "            for i, group in enumerate(optimizer.param_groups):\n",
    "                if 'initial_lr' not in group:\n",
    "                    raise KeyError(\"param 'initial_lr' is not specified \"\n",
    "                                   \"in param_groups[{}] when resuming an optimizer\".format(i))\n",
    "        self.base_lrs = list(map(lambda group: group['initial_lr'], optimizer.param_groups))\n",
    "        self.step(last_epoch + 1)\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "    def get_lr(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "class LambdaLR(_LRScheduler):\n",
    "    \"\"\"Sets the learning rate of each parameter group to the initial lr\n",
    "    times a given function. When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        lr_lambda (function or list): A function which computes a multiplicative\n",
    "            factor given an integer parameter epoch, or a list of such\n",
    "            functions, one for each group in optimizer.param_groups.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer has two groups.\n",
    "        >>> lambda1 = lambda epoch: epoch // 30\n",
    "        >>> lambda2 = lambda epoch: 0.95 ** epoch\n",
    "        >>> scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     scheduler.step()\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, lr_lambda, last_epoch=-1):\n",
    "        self.optimizer = optimizer\n",
    "        if not isinstance(lr_lambda, list) and not isinstance(lr_lambda, tuple):\n",
    "            self.lr_lambdas = [lr_lambda] * len(optimizer.param_groups)\n",
    "        else:\n",
    "            if len(lr_lambda) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"Expected {} lr_lambdas, but got {}\".format(\n",
    "                    len(optimizer.param_groups), len(lr_lambda)))\n",
    "            self.lr_lambdas = list(lr_lambda)\n",
    "        self.last_epoch = last_epoch\n",
    "        super(LambdaLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [base_lr * lmbda(self.last_epoch)\n",
    "                for lmbda, base_lr in zip(self.lr_lambdas, self.base_lrs)]\n",
    "\n",
    "\n",
    "\n",
    "class StepLR(_LRScheduler):\n",
    "    \"\"\"Sets the learning rate of each parameter group to the initial lr\n",
    "    decayed by gamma every step_size epochs. When last_epoch=-1, sets\n",
    "    initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        step_size (int): Period of learning rate decay.\n",
    "        gamma (float): Multiplicative factor of learning rate decay.\n",
    "            Default: 0.1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 0.5 for all groups\n",
    "        >>> # lr = 0.05     if epoch < 30\n",
    "        >>> # lr = 0.005    if 30 <= epoch < 60\n",
    "        >>> # lr = 0.0005   if 60 <= epoch < 90\n",
    "        >>> # ...\n",
    "        >>> scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     scheduler.step()\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, step_size, gamma=0.1, last_epoch=-1):\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        super(StepLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [base_lr * self.gamma ** (self.last_epoch // self.step_size)\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "\n",
    "class MultiStepLR(_LRScheduler):\n",
    "    \"\"\"Set the learning rate of each parameter group to the initial lr decayed\n",
    "    by gamma once the number of epoch reaches one of the milestones. When\n",
    "    last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        milestones (list): List of epoch indices. Must be increasing.\n",
    "        gamma (float): Multiplicative factor of learning rate decay.\n",
    "            Default: 0.1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "\n",
    "    Example:\n",
    "        >>> # Assuming optimizer uses lr = 0.5 for all groups\n",
    "        >>> # lr = 0.05     if epoch < 30\n",
    "        >>> # lr = 0.005    if 30 <= epoch < 80\n",
    "        >>> # lr = 0.0005   if epoch >= 80\n",
    "        >>> scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "        >>> for epoch in range(100):\n",
    "        >>>     scheduler.step()\n",
    "        >>>     train(...)\n",
    "        >>>     validate(...)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, milestones, gamma=0.1, last_epoch=-1):\n",
    "        if not list(milestones) == sorted(milestones):\n",
    "            raise ValueError('Milestones should be a list of'\n",
    "                             ' increasing integers. Got {}', milestones)\n",
    "        self.milestones = milestones\n",
    "        self.gamma = gamma\n",
    "        super(MultiStepLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [base_lr * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ExponentialLR(_LRScheduler):\n",
    "    \"\"\"Set the learning rate of each parameter group to the initial lr decayed\n",
    "    by gamma every epoch. When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        gamma (float): Multiplicative factor of learning rate decay.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, gamma, last_epoch=-1):\n",
    "        self.gamma = gamma\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [base_lr * self.gamma ** self.last_epoch\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ReduceLROnPlateau(object):\n",
    "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
    "    Models often benefit from reducing the learning rate by a factor\n",
    "    of 2-10 once learning stagnates. This scheduler reads a metrics\n",
    "    quantity and if no improvement is seen for a 'patience' number\n",
    "    of epochs, the learning rate is reduced.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        mode (str): One of `min`, `max`. In `min` mode, lr will\n",
    "            be reduced when the quantity monitored has stopped\n",
    "            decreasing; in `max` mode it will be reduced when the\n",
    "            quantity monitored has stopped increasing. Default: 'min'.\n",
    "        factor (float): Factor by which the learning rate will be\n",
    "            reduced. new_lr = lr * factor. Default: 0.1.\n",
    "        patience (int): Number of epochs with no improvement after\n",
    "            which learning rate will be reduced. Default: 10.\n",
    "        verbose (bool): If ``True``, prints a message to stdout for\n",
    "            each update. Default: ``False``.\n",
    "        threshold (float): Threshold for measuring the new optimum,\n",
    "            to only focus on significant changes. Default: 1e-4.\n",
    "        threshold_mode (str): One of `rel`, `abs`. In `rel` mode,\n",
    "            dynamic_threshold = best * ( 1 + threshold ) in 'max'\n",
    "            mode or best * ( 1 - threshold ) in `min` mode.\n",
    "            In `abs` mode, dynamic_threshold = best + threshold in\n",
    "            `max` mode or best - threshold in `min` mode. Default: 'rel'.\n",
    "        cooldown (int): Number of epochs to wait before resuming\n",
    "            normal operation after lr has been reduced. Default: 0.\n",
    "        min_lr (float or list): A scalar or a list of scalars. A\n",
    "            lower bound on the learning rate of all param groups\n",
    "            or each group respectively. Default: 0.\n",
    "        eps (float): Minimal decay applied to lr. If the difference\n",
    "            between new and old lr is smaller than eps, the update is\n",
    "            ignored. Default: 1e-8.\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     train(...)\n",
    "        >>>     val_loss = validate(...)\n",
    "        >>>     # Note that step should be called after validate()\n",
    "        >>>     scheduler.step(val_loss)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, mode='min', factor=0.1, patience=10,\n",
    "                 verbose=False, threshold=1e-4, threshold_mode='rel',\n",
    "                 cooldown=0, min_lr=0, eps=1e-8):\n",
    "\n",
    "        if factor >= 1.0:\n",
    "            raise ValueError('Factor should be < 1.0.')\n",
    "        self.factor = factor\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(min_lr, list) or isinstance(min_lr, tuple):\n",
    "            if len(min_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} min_lrs, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(min_lr)))\n",
    "            self.min_lrs = list(min_lr)\n",
    "        else:\n",
    "            self.min_lrs = [min_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.cooldown = cooldown\n",
    "        self.cooldown_counter = 0\n",
    "        self.mode = mode\n",
    "        self.threshold = threshold\n",
    "        self.threshold_mode = threshold_mode\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = None\n",
    "        self.mode_worse = None  # the worse value for the chosen mode\n",
    "        self.is_better = None\n",
    "        self.eps = eps\n",
    "        self.last_epoch = -1\n",
    "        self._init_is_better(mode=mode, threshold=threshold,\n",
    "                             threshold_mode=threshold_mode)\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Resets num_bad_epochs counter and cooldown counter.\"\"\"\n",
    "        self.best = self.mode_worse\n",
    "        self.cooldown_counter = 0\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, metrics, epoch=None):\n",
    "        current = metrics\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch = self.last_epoch + 1\n",
    "        self.last_epoch = epoch\n",
    "\n",
    "        if self.is_better(current, self.best):\n",
    "            self.best = current\n",
    "            self.num_bad_epochs = 0\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.in_cooldown:\n",
    "            self.cooldown_counter -= 1\n",
    "            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown\n",
    "\n",
    "        if self.num_bad_epochs > self.patience:\n",
    "            self._reduce_lr(epoch)\n",
    "            self.cooldown_counter = self.cooldown\n",
    "            self.num_bad_epochs = 0\n",
    "\n",
    "    def _reduce_lr(self, epoch):\n",
    "        for i, param_group in enumerate(self.optimizer.param_groups):\n",
    "            old_lr = float(param_group['lr'])\n",
    "            new_lr = max(old_lr * self.factor, self.min_lrs[i])\n",
    "            if old_lr - new_lr > self.eps:\n",
    "                param_group['lr'] = new_lr\n",
    "                if self.verbose:\n",
    "                    print('Epoch {:5d}: reducing learning rate'\n",
    "                          ' of group {} to {:.4e}.'.format(epoch, i, new_lr))\n",
    "\n",
    "    @property\n",
    "    def in_cooldown(self):\n",
    "        return self.cooldown_counter > 0\n",
    "\n",
    "    def _init_is_better(self, mode, threshold, threshold_mode):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if threshold_mode not in {'rel', 'abs'}:\n",
    "            raise ValueError('threshold mode ' + mode + ' is unknown!')\n",
    "        if mode == 'min' and threshold_mode == 'rel':\n",
    "            rel_epsilon = 1. - threshold\n",
    "            self.is_better = lambda a, best: a < best * rel_epsilon\n",
    "            self.mode_worse = float('Inf')\n",
    "        elif mode == 'min' and threshold_mode == 'abs':\n",
    "            self.is_better = lambda a, best: a < best - threshold\n",
    "            self.mode_worse = float('Inf')\n",
    "        elif mode == 'max' and threshold_mode == 'rel':\n",
    "            rel_epsilon = threshold + 1.\n",
    "            self.is_better = lambda a, best: a > best * rel_epsilon\n",
    "            self.mode_worse = -float('Inf')\n",
    "        else:  # mode == 'max' and epsilon_mode == 'abs':\n",
    "            self.is_better = lambda a, best: a > best + threshold\n",
    "            self.mode_worse = -float('Inf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1,2,0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "def test(model, num_images=6):\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for i, data in enumerate(dataloaders['val']):\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('predicted: {}'.format(class_names[np.asscalar(preds.cpu().numpy()[j])]))\n",
    "            imshow(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                return        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs = 25):\n",
    "    since = time.time()\n",
    "    \n",
    "\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True) # Set model to traning mode\n",
    "            else:\n",
    "                model.train(False) # Set model to evaluate mode\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            for data in dataloaders[phase]:\n",
    "                inputs, labels = data\n",
    "                \n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # print(\"Inputs {}:\".format(inputs))\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data,1)\n",
    "                # print(\"Outputs {}:\".format(outputs))\n",
    "                # print(\"Labels {}:\".format(labels))\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "                \n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    # loas best weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "# Architecture\n",
    "batch_size = 50\n",
    "depth = 6\n",
    "img_size = 128\n",
    "num_fc = 1\n",
    "out_channels_first_block = 16\n",
    "num_classes = 2\n",
    "in_channels = 3 # RGB\n",
    "\n",
    "# Optimization hyperparameters\n",
    "learning_rate = 0.0005\n",
    "momentum = 0.9\n",
    "step_size = 7\n",
    "gamma = 0.3\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Scale(img_size),\n",
    "        transforms.RandomSizedCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])        \n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(img_size),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "data_dir = 'PeopleVsAnimals'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size, num_workers=4, shuffle=True) for x in ['train','val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"GPU available: {}\".format(use_gpu))\n",
    "\n",
    "\n",
    "model = Net(num_classes, in_channels, depth, num_fc, out_channels_first_block, img_size)\n",
    "\n",
    "\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate, momentum = momentum)\n",
    "exp_lr_scheduler = StepLR(optimizer, step_size = step_size, gamma = gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.0077 Acc: 0.8594\n",
      "val Loss: 0.0203 Acc: 0.5000\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.0053 Acc: 0.8914\n",
      "val Loss: 0.0115 Acc: 0.6500\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.0036 Acc: 0.9350\n",
      "val Loss: 0.0046 Acc: 0.9433\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 0.9555\n",
      "val Loss: 0.0038 Acc: 0.9467\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.9608\n",
      "val Loss: 0.0057 Acc: 0.9067\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.9675\n",
      "val Loss: 0.0034 Acc: 0.9467\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.0019 Acc: 0.9715\n",
      "val Loss: 0.0027 Acc: 0.9633\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9702\n",
      "val Loss: 0.0038 Acc: 0.9467\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.0018 Acc: 0.9729\n",
      "val Loss: 0.0035 Acc: 0.9467\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.0016 Acc: 0.9742\n",
      "val Loss: 0.0022 Acc: 0.9633\n",
      "\n",
      "Training complete in 3m 20s\n",
      "Best val Acc: 0.963333\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
